{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSiVgp7hKsEn99mrAtVpl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arian-Space/Cortex_V0.1_proyect/blob/main/Bio_Inspired_Modular_Architecture_for_Efficient_Memory_and_Reasoning_in_Neural_Networks_Preliminary_Results_of_a_Compact_Prototype_%22Cortex_V0_1%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cortex-V0.1: Bio-Inspired Modular Architecture for Efficient Memory & Reasoning"
      ],
      "metadata": {
        "id": "22M-NA9RFnoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compact Prototype on SQuAD (Preliminary Results)"
      ],
      "metadata": {
        "id": "cOaeOUA6Fpn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author**: Arian Vazquez Fernandez  \n",
        "**License**: Apache 2.0  \n",
        "**Date**: January 2026  \n",
        "\n",
        "This notebook implements the two-phase bio-inspired architecture described in the paper:  \n",
        "- **Phase 1**: Topological routing + modern Hopfield memory (frozen after training)  \n",
        "- **Phase 2**: Dynamic reasoning with Mixture of Experts (MoE), with optional ablation (dense MLP)  \n",
        "\n",
        "All experiments run on free Colab GPU (T4 recommended). Uses Weights & Biases (wandb) for logging & sweeps."
      ],
      "metadata": {
        "id": "Q2hYAapbFt0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup & Dependencies"
      ],
      "metadata": {
        "id": "AgccLuAWF3LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages (quiet mode) and import everything at once."
      ],
      "metadata": {
        "id": "efwnnlk9F4M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Install Dependencies ===\n",
        "!pip install torch wandb sentence-transformers datasets scikit-learn matplotlib --quiet\n",
        "\n",
        "# === 2. Imports ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import wandb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to Weights & Biases (store your key in Colab secrets)\n",
        "wandb.login(key=userdata.get('WANDB-KEY'))\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "metadata": {
        "id": "HXI_y9FIFtTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Load & Preprocess Data (SQuAD subset)"
      ],
      "metadata": {
        "id": "cJSDRK4vGH6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocessing step is crucial for the bio-inspired separation of concerns:\n",
        "\n",
        "* Unique contexts serve as \"long-term memories\" stored in the Hopfield module (Phase 1), mimicking hippocampal pattern completion.\n",
        "\n",
        "* Expanded answer windows provide richer semantic targets for reasoning (Phase 2), simulating cortical integration of partial recall.\n",
        "The simple keyword-based classification into \"math\", \"history\", and \"facts\" is not used for supervision during training — it is only collected for post-hoc visualization of emergent expert specialization via PCA. This allows us to observe whether experts naturally cluster by semantic domain without explicit labels, a key indicator of distributed, bio-like representation learning."
      ],
      "metadata": {
        "id": "CRt3Fj9FGJuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XiAdj9TppA-"
      },
      "outputs": [],
      "source": [
        "# === Load Dataset ===\n",
        "print(\"Loading SQuAD subset (first 10k samples)...\")\n",
        "dataset = load_dataset(\"squad\", split=\"train[:10000]\")\n",
        "\n",
        "# Phase 1: Unique contexts for memory\n",
        "contexts = list(set(item['context'] for item in dataset))\n",
        "print(f\"{len(contexts)} unique contexts for Hopfield memory.\")\n",
        "\n",
        "# Embed contexts (frozen embedder)\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "with torch.no_grad():\n",
        "    context_embs = embed_model.encode(contexts, batch_size=64, convert_to_tensor=True, device=device)\n",
        "    context_embs = F.normalize(context_embs, dim=-1)\n",
        "\n",
        "# Train/eval split for memory patterns\n",
        "num_train = int(len(context_embs) * 0.8)\n",
        "context_embs_train = context_embs[:num_train]\n",
        "context_embs_eval = context_embs[num_train:]\n",
        "\n",
        "# Phase 2: QA pairs with expanded answers\n",
        "qa_data = []\n",
        "for item in dataset:\n",
        "    answer_text = item['answers']['text'][0]\n",
        "    context = item['context']\n",
        "    start = context.find(answer_text)\n",
        "    if start != -1:\n",
        "        window_start = max(0, start - 60)\n",
        "        window_end = min(len(context), start + len(answer_text) + 60)\n",
        "        long_answer = context[window_start:window_end].strip()\n",
        "    else:\n",
        "        long_answer = f\"The answer is {answer_text}.\"\n",
        "    qa_data.append({\n",
        "        \"question\": item['question'],\n",
        "        \"answer\": long_answer\n",
        "    })\n",
        "\n",
        "# Simple question type classification (for PCA visualization only)\n",
        "def classify_type(q):\n",
        "    q = q.lower()\n",
        "    math_keywords = ['how many', 'what is', 'calculate', 'sum', 'difference', 'product', 'square', 'root', 'area', 'perimeter', 'angle', 'prime']\n",
        "    history_keywords = ['who', 'when', 'year', 'first', 'last', 'president', 'war', 'battle', 'discovered', 'invented', 'born', 'died']\n",
        "    if any(kw in q for kw in math_keywords):\n",
        "        return \"math\"\n",
        "    if any(kw in q for kw in history_keywords):\n",
        "        return \"history\"\n",
        "    return \"facts\"\n",
        "\n",
        "for d in qa_data:\n",
        "    d['type'] = classify_type(d['question'])\n",
        "\n",
        "print(f\"QA data ready: {len(qa_data)} examples classified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Definitions"
      ],
      "metadata": {
        "id": "Tbiplp7TGv4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Phase 1: Topological Routing + Modern Hopfield Memory"
      ],
      "metadata": {
        "id": "DRvmDgzgG0H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The routing layers (input_proj → gray → brown → blue_proj) gradually transform the raw question embedding into a query optimized for associative retrieval. This multi-stage processing with residual connections (gray) and recurrent refinement (brown GRU) imitates the hierarchical feature extraction and temporal binding found in biological sensory pathways. The modern Hopfield module itself has no trainable parameters — it acts purely as a content-addressable memory buffer with exponential storage capacity (as theoretically shown in Krotov & Hopfield, 2016; Ramsauer et al., 2020), enabling reliable pattern completion even from noisy or partial cues."
      ],
      "metadata": {
        "id": "wnL_eN2pIGuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModernHopfield(nn.Module):\n",
        "    \"\"\"Modern Hopfield memory module (buffer-based, no trainable params).\"\"\"\n",
        "    def __init__(self, dim=384, max_patterns=10000, beta=20.0):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.beta = beta\n",
        "        self.register_buffer('patterns', torch.zeros(max_patterns, dim))\n",
        "        self.num_stored = 0\n",
        "\n",
        "    def store(self, new_patterns):\n",
        "        n = new_patterns.shape[0]\n",
        "        self.patterns[self.num_stored:self.num_stored + n] = new_patterns.float()\n",
        "        self.num_stored += n\n",
        "\n",
        "    def forward(self, query, top_k=64):\n",
        "        patterns_active = self.patterns[:self.num_stored].float()\n",
        "        logits = torch.matmul(query.float(), patterns_active.t()) / (self.dim ** 0.5)\n",
        "        logits = self.beta * logits\n",
        "        attn = F.softmax(logits, dim=-1)\n",
        "        effective_k = min(top_k, self.num_stored)\n",
        "        topk_weights, topk_idx = torch.topk(attn, effective_k, dim=-1)\n",
        "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
        "        selected = patterns_active[topk_idx]\n",
        "        retrieved = (topk_weights.unsqueeze(-1) * selected).sum(dim=1)\n",
        "        return retrieved, topk_weights, topk_idx, logits\n",
        "\n",
        "\n",
        "class MultimodalModel(nn.Module):\n",
        "    \"\"\"Phase 1 full model: Routing layers + Hopfield memory.\"\"\"\n",
        "    def __init__(self, embed_dim=384, short_mem_dim=384, hopfield_dim=384, top_k=64, beta=20.0):\n",
        "        super().__init__()\n",
        "        self.top_k = top_k\n",
        "        self.input_proj = nn.Linear(embed_dim, short_mem_dim)\n",
        "        self.gray = nn.Sequential(\n",
        "            nn.Linear(short_mem_dim, short_mem_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(short_mem_dim * 2, short_mem_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.brown = nn.GRU(short_mem_dim, short_mem_dim, num_layers=3, batch_first=True, dropout=0.2)\n",
        "        self.brown_norm = nn.LayerNorm(short_mem_dim)\n",
        "        self.blue_proj = nn.Linear(short_mem_dim, hopfield_dim)\n",
        "        self.violet = SimpleModernHopfield(dim=hopfield_dim, max_patterns=len(contexts), beta=beta)\n",
        "\n",
        "    def forward(self, queries):\n",
        "        x = self.input_proj(queries)\n",
        "        mod = self.gray(x)\n",
        "        x = x + mod\n",
        "        x_short, _ = self.brown(x.unsqueeze(1))\n",
        "        x_short = self.brown_norm(x_short.squeeze(1))\n",
        "        query = self.blue_proj(x_short)\n",
        "        retrieved, topk_weights, topk_idx, logits = self.violet(query, top_k=self.top_k)\n",
        "        return retrieved, topk_weights, topk_idx, logits"
      ],
      "metadata": {
        "id": "tTXc1tPdG3zw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Phase 2: Dynamic Reasoning (MoE or Ablation MLP)"
      ],
      "metadata": {
        "id": "Wu4zEN1GG1wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mixture of Experts layer introduces conditional computation and dynamic sparsity: only a small subset of experts (top-k) is activated per input, drastically reducing FLOPs while allowing specialization. This mirrors biological cortical columns where different neuronal populations handle distinct cognitive functions. The router learns to assign experts implicitly based on the retrieved memory vector, fostering emergent division of labor. The ablation (dense MLP) serves as a strong baseline to quantify the benefit of this sparse, modular routing — demonstrating whether specialization truly emerges from the interaction between noisy retrieval and expert gating."
      ],
      "metadata": {
        "id": "-lPqKUtlIMq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicReasoningLayer(nn.Module):\n",
        "    \"\"\"Mixture of Experts layer for dynamic reasoning.\"\"\"\n",
        "    def __init__(self, dim=384, num_experts=16, expert_layers=4, top_k=4):\n",
        "        super().__init__()\n",
        "        self.router = nn.Linear(dim, num_experts)\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(*[nn.Sequential(nn.Linear(dim, dim), nn.GELU(), nn.Dropout(0.1)) for _ in range(expert_layers)])\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, retrieved):\n",
        "        gate_scores = F.softmax(self.router(retrieved), dim=-1)\n",
        "        topk_vals, topk_idx = torch.topk(gate_scores, self.top_k, dim=-1)\n",
        "        outputs = torch.zeros_like(retrieved)\n",
        "        for i in range(self.top_k):\n",
        "            weight = topk_vals[:, i].unsqueeze(-1)\n",
        "            expert_idx = topk_idx[:, i]\n",
        "            selected_out = torch.stack([self.experts[idx](retrieved[b:b+1]).squeeze(0) for b, idx in enumerate(expert_idx)])\n",
        "            outputs += weight * selected_out\n",
        "        return outputs, gate_scores.detach()\n",
        "\n",
        "\n",
        "class AblationMLP(nn.Module):\n",
        "    \"\"\"Dense MLP ablation (equivalent capacity to MoE).\"\"\"\n",
        "    def __init__(self, dim=384):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(*[nn.Sequential(nn.Linear(dim, dim), nn.GELU()) for _ in range(16)])\n",
        "\n",
        "    def forward(self, retrieved):\n",
        "        return self.net(retrieved), None\n",
        "\n",
        "\n",
        "class Phase2Model(nn.Module):\n",
        "    \"\"\"Full Phase 2 model: Frozen Phase 1 + Reasoning layer.\"\"\"\n",
        "    def __init__(self, phase1_model, num_experts=16, expert_layers=4, top_k=4, ablation=False):\n",
        "        super().__init__()\n",
        "        self.phase1 = phase1_model\n",
        "        if ablation:\n",
        "            self.reasoning = AblationMLP()\n",
        "        else:\n",
        "            self.reasoning = DynamicReasoningLayer(num_experts=num_experts, expert_layers=expert_layers, top_k=top_k)\n",
        "\n",
        "    def forward(self, query_emb):\n",
        "        with torch.no_grad():\n",
        "            retrieved, _, _, _ = self.phase1(query_emb)\n",
        "        reasoned, gate_scores = self.reasoning(retrieved)\n",
        "        return reasoned, gate_scores"
      ],
      "metadata": {
        "id": "eQzF-uMmG9zh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Datasets for Training"
      ],
      "metadata": {
        "id": "DCdz68EvHDqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both datasets are designed for efficiency and reproducibility on Colab T4 GPUs:\n",
        "\n",
        "* Phase 1 uses memory patterns directly (no labels needed beyond self-supervision via cosine + ranking loss).\n",
        "* Phase 2 uses frozen embeddings to avoid token-level generation overhead, focusing purely on semantic vector alignment — a practical choice for rapid prototyping while still capturing high-level reasoning quality via cosine similarity."
      ],
      "metadata": {
        "id": "s1ZTiY8nITEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 1 Dataset (memory patterns with optional noise)\n",
        "class ContextDataset(Dataset):\n",
        "    def __init__(self, embs, is_eval=False, noise_level=0.1):\n",
        "        self.embs = embs\n",
        "        self.is_eval = is_eval\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        emb = self.embs[idx]\n",
        "        if self.is_eval:\n",
        "            emb = emb + self.noise_level * torch.randn_like(emb)\n",
        "            emb = F.normalize(emb, dim=-1)\n",
        "        return emb, idx\n",
        "\n",
        "\n",
        "# Phase 2 Dataset (QA pairs + types)\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, qa_list, embed_model, device):\n",
        "        self.query_embs = embed_model.encode([d[\"question\"] for d in qa_list], convert_to_tensor=True, device=device)\n",
        "        self.query_embs = F.normalize(self.query_embs, dim=-1)\n",
        "        self.answer_embs = embed_model.encode([d[\"answer\"] for d in qa_list], convert_to_tensor=True, device=device)\n",
        "        self.answer_embs = F.normalize(self.answer_embs, dim=-1)\n",
        "        self.types = [d[\"type\"] for d in qa_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.query_embs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.query_embs[i], self.answer_embs[i], self.types[i]"
      ],
      "metadata": {
        "id": "kkLqpH-DHFYb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training Phase 1 (Topological Routing + Memory)"
      ],
      "metadata": {
        "id": "gITxuq6THKJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training is self-supervised: the model learns to map noisy/partial queries back to clean context embeddings. The combined loss (cosine similarity + ranking cross-entropy) encourages both accurate retrieval and correct ranking of top candidates. After 30 epochs, the routing layers are frozen, preserving the learned associative pathways while preventing catastrophic interference during Phase 2."
      ],
      "metadata": {
        "id": "99legTFLIeCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_phase1_simple():\n",
        "    \"\"\"Train Phase 1: Routing layers only (Hopfield is buffer).\"\"\"\n",
        "    wandb.init(project=\"cortex-squad-phase1-final\", name=\"run-final-dim384\")\n",
        "\n",
        "    model = MultimodalModel().to(device)\n",
        "    model.violet.store(context_embs_train.to(device))\n",
        "\n",
        "    loader = DataLoader(ContextDataset(context_embs_train), batch_size=32, shuffle=True)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for queries, target_idx in loader:\n",
        "            queries = queries.to(device)\n",
        "            target_idx = target_idx.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            retrieved, _, topk_idx, logits = model(queries)\n",
        "            target = context_embs_train[target_idx].to(device)\n",
        "            loss_cos = 1 - F.cosine_similarity(retrieved, target).mean()\n",
        "            loss_rank = 0.1 * F.cross_entropy(logits, target_idx)\n",
        "            loss = loss_cos + loss_rank\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"Phase 1 Epoch {epoch+1:2d} | Loss: {avg_loss:.4f}\")\n",
        "        wandb.log({\"phase1_loss\": avg_loss})\n",
        "\n",
        "    torch.save(model.state_dict(), \"phase1_final_dim384.pth\")\n",
        "    print(\"Phase 1 training completed and saved.\")\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "wB-BqmjZHMAo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Training Phase 2 (Dynamic Reasoning with Sweep)"
      ],
      "metadata": {
        "id": "hna7YHhKHOKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phase is the core of the paper's novelty: by freezing Phase 1, we force the reasoning module to develop robustness to imperfect memory recall — a key bio-inspired property. The MoE structure must compensate for noisy or approximate retrieved vectors, leading to emergent semantic specialization (visible in PCA plots). Wandb sweeps systematically explore the hyperparameter space, allowing reliable comparison between MoE variants and the dense MLP ablation. Gate scores are accumulated per question type for visualization, confirming that specialization arises implicitly (no type labels in loss).\n",
        "\n",
        "The PCA plots (logged every 10 epochs) are a highlight: they show progressive cluster separation from uniform distribution (epoch 0) to clear semantic groups (epoch 40), providing visual evidence of distributed, brain-like organization emerging in a compact model (<15M trainable params total)."
      ],
      "metadata": {
        "id": "5i8xg_UBIhaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_phase2():\n",
        "    \"\"\"Train Phase 2: Only reasoning layer (Phase 1 frozen).\"\"\"\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "        run.name = f\"lr_{config.lr}-bs_{config.batch_size}-experts_{config.num_experts}-layers_{config.expert_layers}-topk_{config.top_k}-ablation_{config.ablation}\"\n",
        "\n",
        "        # Load frozen Phase 1\n",
        "        phase1_model = MultimodalModel().to(device)\n",
        "        phase1_model.load_state_dict(torch.load(\"phase1_final_dim384.pth\"))\n",
        "        phase1_model.eval()\n",
        "        for p in phase1_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        phase1_model.violet.store(context_embs_train.to(device))\n",
        "\n",
        "        model = Phase2Model(phase1_model,\n",
        "                            num_experts=config.num_experts,\n",
        "                            expert_layers=config.expert_layers,\n",
        "                            top_k=config.top_k,\n",
        "                            ablation=config.ablation).to(device)\n",
        "\n",
        "        optimizer = AdamW(model.reasoning.parameters(), lr=config.lr)\n",
        "\n",
        "        dataset = QADataset(qa_data, embed_model, device)\n",
        "        loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "        exact_threshold = 0.9\n",
        "        gate_accum = {\"math\": [], \"history\": [], \"facts\": []}  # List of (num_experts,) arrays\n",
        "\n",
        "        for epoch in range(50):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            total_sim = 0\n",
        "            total_exact = 0\n",
        "\n",
        "            for q_emb, a_emb, q_type in loader:\n",
        "                q_emb, a_emb = q_emb.to(device), a_emb.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                reasoned, gate_scores = model(q_emb)\n",
        "                loss = 1 - F.cosine_similarity(reasoned, a_emb).mean()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                sim = F.cosine_similarity(reasoned, a_emb)\n",
        "                total_loss += loss.item() * q_emb.size(0)\n",
        "                total_sim += sim.sum().item()\n",
        "                total_exact += (sim > exact_threshold).sum().item()\n",
        "\n",
        "                # Safe gate accumulation\n",
        "                if gate_scores is not None:\n",
        "                    gate_scores = gate_scores.detach().cpu()\n",
        "                    for t in set(q_type):\n",
        "                        mask = [typ == t for typ in q_type]\n",
        "                        if any(mask):\n",
        "                            indices = [i for i, m in enumerate(mask) if m]\n",
        "                            mean_gate = gate_scores[indices].mean(0).numpy()  # (num_experts,)\n",
        "                            gate_accum[t].append(mean_gate)\n",
        "\n",
        "            avg_loss = total_loss / len(dataset)\n",
        "            avg_sim = total_sim / len(dataset)\n",
        "            exact_rate = total_exact / len(dataset)\n",
        "\n",
        "            wandb.log({\n",
        "                \"loss\": avg_loss,\n",
        "                \"close_answer_mean\": avg_sim,\n",
        "                \"exact_match\": exact_rate\n",
        "            })\n",
        "            print(f\"Epoch {epoch+1:2d} | Loss: {avg_loss:.4f} | Close: {avg_sim:.4f} | Exact: {exact_rate:.4f}\")\n",
        "\n",
        "            # PCA visualization every 10 epochs (robust)\n",
        "            if not config.ablation and epoch % 10 == 0 and len(gate_accum[\"math\"]) > 0:\n",
        "                valid_types = [t for t in gate_accum if len(gate_accum[t]) > 0]\n",
        "                if len(valid_types) >= 2:\n",
        "                    mean_gates = {}\n",
        "                    for t in valid_types:\n",
        "                        gates_array = np.stack(gate_accum[t])  # (n_epochs, num_experts)\n",
        "                        mean_gates[t] = np.mean(gates_array, axis=0)\n",
        "                    points = np.array([mean_gates[t] for t in valid_types])\n",
        "                    pca = PCA(n_components=2)\n",
        "                    pca_points = pca.fit_transform(points)\n",
        "\n",
        "                    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "                    colors = {\"math\": \"blue\", \"history\": \"red\", \"facts\": \"green\"}\n",
        "                    for i, t in enumerate(valid_types):\n",
        "                        ax.scatter(pca_points[i,0], pca_points[i,1], c=colors[t], label=t, s=200)\n",
        "                        ax.text(pca_points[i,0]+0.02, pca_points[i,1], t, fontsize=14)\n",
        "                    ax.set_title(f\"Expert Specialization - Epoch {epoch}\")\n",
        "                    ax.set_xlabel(\"PCA Component 1\")\n",
        "                    ax.set_ylabel(\"PCA Component 2\")\n",
        "                    ax.legend()\n",
        "                    wandb.log({\"expert_specialization_2d\": wandb.Image(fig)})\n",
        "                    plt.close(fig)\n",
        "\n",
        "        print(\"Phase 2 training completed.\")\n",
        "        wandb.finish()"
      ],
      "metadata": {
        "id": "65WBH-EmHOxd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Run Experiments"
      ],
      "metadata": {
        "id": "vOCdB62THTv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow summary:\n",
        "\n",
        "1. Run Phase 1 once → generates a frozen routing + memory checkpoint.\n",
        "2. Launch wandb sweep for Phase 2 → automatically tests dozens of MoE configurations + ablations.\n",
        "3. Monitor in wandb dashboard: loss curves, close_answer_mean (semantic quality), exact_match, and especially the evolving PCA plots for expert specialization.\n",
        "\n",
        "This modular, two-phase training + freezing strategy is computationally cheap (fits in free Colab), reproducible, and directly supports the paper's claims of extreme efficiency and cognitive robustness in limited-resource settings."
      ],
      "metadata": {
        "id": "wAETled8HUTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run Phase 1 (run this once) ===\n",
        "train_phase1_simple()\n",
        "\n",
        "# === Sweep configuration for Phase 2 ===\n",
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
        "    \"parameters\": {\n",
        "        \"lr\": {\"values\": [1e-3, 5e-4, 1e-4]},\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"num_experts\": {\"values\": [8, 16, 32]},\n",
        "        \"expert_layers\": {\"values\": [2, 4, 6]},\n",
        "        \"top_k\": {\"values\": [2, 4, 8]},\n",
        "        \"ablation\": {\"values\": [False, True]}  # Include ablation for comparison\n",
        "    }\n",
        "}\n",
        "\n",
        "# Launch sweep (adjust count as needed)\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"cortex-fase2-community\")\n",
        "wandb.agent(sweep_id, function=train_phase2, count=30)  # Run 30 configurations"
      ],
      "metadata": {
        "id": "DBWFboH-HWbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}